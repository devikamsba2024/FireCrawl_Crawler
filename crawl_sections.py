#!/usr/bin/env python3
"""
Crawl specific sections of the website independently.
Useful for scheduling different sections with different frequencies.
"""
import argparse
import json
import sys
from pathlib import Path
from firecrawl_crawler import Config, FirecrawlClient, MarkdownStorage
from firecrawl_crawler.logger import get_logger

logger = get_logger(__name__)


def load_sections_config(config_file: str = "sections_config.json"):
    """Load sections configuration."""
    config_path = Path(config_file)
    if not config_path.exists():
        print(f"Error: Config file {config_file} not found")
        sys.exit(1)
    
    return json.loads(config_path.read_text())


def list_sections(sections_config):
    """List all available sections."""
    print("\nAvailable Sections:")
    print("=" * 80)
    
    for key, section in sections_config["sections"].items():
        max_depth = section.get('max_depth', 'auto')
        limit = section.get('limit', 'auto')
        timeout = section.get('timeout', 'auto')
        
        print(f"\n{section['name']} ({key})")
        print(f"  URL: {section['url']}")
        print(f"  Output: {section['output_dir']}")
        print(f"  Limits: max_depth={max_depth}, limit={limit}, timeout={timeout if timeout != 'auto' else 'auto'}s")
        if max_depth == 'auto' or limit == 'auto' or timeout == 'auto':
            print(f"  ‚ö†Ô∏è  Auto-detection: Will analyze sitemap to determine values")
        print(f"  Schedule: {section['schedule']}")
        print(f"  Description: {section['description']}")


def crawl_section(section_key, sections_config, api_url=None, api_key=None, force=False):
    """Crawl a specific section."""
    if section_key not in sections_config["sections"]:
        print(f"Error: Section '{section_key}' not found")
        print(f"Available sections: {', '.join(sections_config['sections'].keys())}")
        sys.exit(1)
    
    section = sections_config["sections"][section_key]
    
    # Auto-detect limit, depth, and timeout from sitemap if not set
    max_depth = section.get('max_depth')
    limit = section.get('limit')
    timeout = section.get('timeout')
    
    if max_depth is None or limit is None or timeout is None:
        print(f"\nüîç Auto-detecting crawl parameters from sitemap...")
        from firecrawl_crawler import SitemapParser
        
        # Parse base URL for sitemap
        from urllib.parse import urlparse
        parsed = urlparse(section['url'])
        base_url = f"{parsed.scheme}://{parsed.netloc}"
        
        parser = SitemapParser(base_url)
        analysis = parser.analyze_section(section['url'])
        
        if analysis['page_count'] > 0:
            if max_depth is None:
                max_depth = analysis['max_depth']
                print(f"  ‚úì Detected max_depth: {max_depth}")
            if limit is None:
                limit = analysis['page_count']
                print(f"  ‚úì Detected page count: {limit}")
            if timeout is None:
                # Calculate timeout based on page count and depth
                # Formula: base_time + (pages * time_per_page) + (depth * depth_factor)
                base_time = 30  # 30 seconds base time
                time_per_page = 3  # 3 seconds per page (conservative estimate)
                depth_factor = 10  # 10 seconds per depth level
                
                calculated_timeout = base_time + (limit * time_per_page) + (max_depth * depth_factor)
                # Add 50% buffer for safety
                timeout = int(calculated_timeout * 1.5)
                # Minimum 60 seconds, maximum 3600 seconds (1 hour)
                timeout = max(60, min(timeout, 3600))
                print(f"  ‚úì Calculated timeout: {timeout}s ({timeout//60}m {timeout%60}s)")
        else:
            print(f"  ‚ö†Ô∏è  No pages found in sitemap, using defaults")
            if max_depth is None:
                max_depth = 2
            if limit is None:
                limit = 50
            if timeout is None:
                timeout = 600  # Default 10 minutes
        print()
    
    print(f"\n{'='*80}")
    print(f"Crawling: {section['name']}")
    print(f"{'='*80}")
    print(f"URL: {section['url']}")
    print(f"Output: {section['output_dir']}")
    print(f"Max Depth: {max_depth}, Limit: {limit}")
    print(f"Timeout: {timeout}s ({timeout//60}m {timeout%60}s)")
    print()
    
    # Setup config
    config = Config(
        api_url=api_url,
        api_key=api_key,
        output_dir=section['output_dir']
    )
    
    client = FirecrawlClient(config)
    storage = MarkdownStorage(config.output_dir)
    
    try:
        # Start crawl (use auto-detected values if available)
        job_id = client.crawl_website(
            url=section['url'],
            max_depth=max_depth,
            limit=limit,
            formats=["markdown"],
            only_main_content=True
        )
        
        print(f"Crawl job started: {job_id}")
        print("Waiting for crawl to complete...\n")
        
        # Wait for completion with configured timeout (already set above)
        result = client.wait_for_crawl(
            job_id=job_id,
            max_wait_time=timeout,
            poll_interval=5
        )
        
        # Save pages
        pages = result.get("data", [])
        if pages:
            print(f"\nCrawl completed! Found {len(pages)} pages.")
            print("Saving pages...\n")
            saved_files = storage.save_multiple_pages(pages, create_index=True)
            print(f"\n‚úì Successfully saved {len(saved_files)} pages to: {config.output_dir}")
            print(f"‚úì Metadata saved to: {config.output_dir}/.scrape_metadata.json")
        else:
            status = result.get("status", "unknown")
            total_pages = result.get("total", 0)
            stats = result.get("stats", {})
            error = result.get("error")
            # Use the job_id from crawl start, fallback to result if needed
            retry_job_id = result.get("id") or result.get("jobId") or job_id
            
            print(f"\n‚ö†Ô∏è  Warning: Crawl completed with status '{status}' but no pages found.")
            print(f"  Job ID: {retry_job_id}")
            print(f"  API URL: {config.api_url}")
            print(f"  Response keys: {list(result.keys())}")
            
            if total_pages > 0:
                print(f"  ‚ö†Ô∏è  API reports {total_pages} total pages, but data array is empty.")
                print(f"      This indicates a Firecrawl API issue - data not ready or lost.")
            elif total_pages == 0:
                print(f"  ‚ö†Ô∏è  API reports 0 total pages - crawl may have found no matching pages.")
            
            if error:
                print(f"  ‚ùå API Error field: {error}")
            
            if stats:
                print(f"  Stats: {stats}")
            
            # Log full response for debugging (especially useful for VM troubleshooting)
            logger.warning(f"Full API response (no data): {json.dumps(result, indent=2, default=str)}")
            print(f"\n  üîç Full API response logged to: logs/crawler.log (DEBUG level)")
            
            # Check if we can get the job status again (might have data now)
            if retry_job_id and retry_job_id != "unknown":
                print(f"\nüîÑ Attempting to fetch job status again (data might be ready now)...")
                try:
                    retry_result = client.get_crawl_status(retry_job_id)
                    retry_pages = retry_result.get("data", [])
                    if retry_pages:
                        print(f"  ‚úì Found {len(retry_pages)} pages on retry!")
                        print("Saving pages...\n")
                        saved_files = storage.save_multiple_pages(retry_pages, create_index=True)
                        print(f"\n‚úì Successfully saved {len(saved_files)} pages to: {config.output_dir}")
                        print(f"‚úì Metadata saved to: {config.output_dir}/.scrape_metadata.json")
                    else:
                        print(f"  ‚ö†Ô∏è  Still no pages in data array")
                        print("\nThis might happen if:")
                        print("  - The crawl is still processing (check again later)")
                        print("  - No pages matched the crawl criteria (depth/limit)")
                        print("  - The website structure changed")
                        print("  - Network/API connectivity issues from VM")
                        print(f"\nüí° Suggestions for VM troubleshooting:")
                        print(f"  - Check logs: tail -f logs/crawler.log")
                        print(f"  - Verify Firecrawl API is accessible: curl {config.api_url}/health")
                        print(f"  - Check if API URL is correct (should be accessible from VM)")
                        print(f"  - Try testing with single page: python3 main.py scrape {section['url']}")
                        print(f"  - Check network connectivity: ping/curl to {config.api_url}")
                        print(f"  - Manually check job status: curl {config.api_url}/v1/crawl/{retry_job_id}")
                        print(f"  - Try running again: python3 crawl_sections.py crawl {section_key}")
                        logger.warning(f"Crawl completed but no pages found for {section['url']}")
                        logger.debug(f"Full result: {result}")
                        logger.debug(f"Retry result: {retry_result}")
                except Exception as e:
                    print(f"  ‚úó Error checking job status: {e}")
                    print(f"\nüí° You can manually check the job status later:")
                    print(f"  Job ID: {retry_job_id}")
                    print(f"  API URL: {config.api_url}/v1/crawl/{retry_job_id}")
            else:
                print(f"\n‚ö†Ô∏è  Cannot retry: Job ID not available")
                print(f"  Original job_id variable: {job_id if 'job_id' in locals() else 'not found'}")
                print(f"  Result keys: {list(result.keys())}")
                logger.warning(f"Crawl completed but no pages found and job ID unavailable for {section['url']}")
                logger.debug(f"Full result: {result}")
            # Don't exit with error - allow it to continue (might be temporary)
            
    except Exception as e:
        print(f"\n‚úó Error: {str(e)}")
        sys.exit(1)


def update_section(section_key, sections_config, api_url=None, api_key=None, auto_update=False, show_urls=False):
    """Check for updates in a specific section."""
    if section_key not in sections_config["sections"]:
        print(f"Error: Section '{section_key}' not found")
        sys.exit(1)
    
    section = sections_config["sections"][section_key]
    
    print(f"\n{'='*80}")
    print(f"Checking Updates: {section['name']}")
    print(f"{'='*80}")
    print(f"URL: {section['url']}")
    print(f"Output: {section['output_dir']}\n")
    
    # Use the main.py update functionality
    from main import check_updates
    
    class Args:
        def __init__(self):
            self.url = section['url']
            self.output = section['output_dir']
            self.api_url = api_url
            self.api_key = api_key
            self.show_urls = show_urls
            self.auto_update = auto_update
            self.full_content = False
    
    args = Args()
    check_updates(args)


def crawl_all_sections(sections_config, api_url=None, api_key=None):
    """Crawl all sections sequentially."""
    total_sections = len(sections_config["sections"])
    
    print(f"\n{'='*80}")
    print(f"Crawling ALL {total_sections} Sections")
    print(f"{'='*80}\n")
    
    for i, (key, section) in enumerate(sections_config["sections"].items(), 1):
        print(f"\n[{i}/{total_sections}] Starting section: {section['name']}")
        crawl_section(key, sections_config, api_url, api_key)
        print(f"\n[{i}/{total_sections}] Completed: {section['name']}")


def main():
    parser = argparse.ArgumentParser(
        description="Crawl website sections independently",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # List all available sections
  python crawl_sections.py list
  
  # Crawl a specific section
  python crawl_sections.py crawl admissions
  python crawl_sections.py crawl graduate_school
  
  # Check for updates in a section
  python crawl_sections.py update admissions --show-urls
  python crawl_sections.py update admissions --auto-update
  
  # Crawl all sections
  python crawl_sections.py crawl-all
  
  # Use with custom config file
  python crawl_sections.py crawl admissions --config my_sections.json
        """
    )
    
    parser.add_argument(
        "--config",
        default="sections_config.json",
        help="Path to sections config file (default: sections_config.json)"
    )
    parser.add_argument(
        "--api-url",
        default=None,
        help="Firecrawl API URL"
    )
    parser.add_argument(
        "--api-key",
        default=None,
        help="Firecrawl API key"
    )
    
    subparsers = parser.add_subparsers(dest="command", help="Command to run")
    subparsers.required = True
    
    # List command
    list_parser = subparsers.add_parser("list", help="List all available sections")
    
    # Crawl command
    crawl_parser = subparsers.add_parser("crawl", help="Crawl a specific section")
    crawl_parser.add_argument("section", help="Section key to crawl")
    crawl_parser.add_argument("--force", action="store_true", help="Force re-crawl even if recent")
    
    # Update command
    update_parser = subparsers.add_parser("update", help="Check for updates in a section")
    update_parser.add_argument("section", help="Section key to check")
    update_parser.add_argument("--show-urls", action="store_true", help="Show updated URLs")
    update_parser.add_argument("--auto-update", action="store_true", help="Automatically update changed pages")
    
    # Crawl all command
    crawl_all_parser = subparsers.add_parser("crawl-all", help="Crawl all sections")
    
    args = parser.parse_args()
    
    # Load config
    sections_config = load_sections_config(args.config)
    
    # Execute command
    if args.command == "list":
        list_sections(sections_config)
    elif args.command == "crawl":
        crawl_section(args.section, sections_config, args.api_url, args.api_key, args.force)
    elif args.command == "update":
        update_section(args.section, sections_config, args.api_url, args.api_key, args.auto_update, args.show_urls)
    elif args.command == "crawl-all":
        crawl_all_sections(sections_config, args.api_url, args.api_key)


if __name__ == "__main__":
    main()

